{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to create a bioinformatics workflow using Snakemake. The workflow includes preprocessing raw sequencing data, performing an analysis, and generating a report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install snakemake\n",
    "! sudo apt-get install fastqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Structure Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "./\n",
    "    |- src/\n",
    "        |- scripts/\n",
    "            |- preprocessing/\n",
    "                |- preprocess.py\n",
    "            |- analysis/\n",
    "                |- analyze.py\n",
    "            |- postprocessing/\n",
    "                |- postprocess.py\n",
    "            |- utilities/\n",
    "                |- log.py\n",
    "                |- report.py\n",
    "    |- data/\n",
    "        |- raw/\n",
    "            |- sample1.fastq\n",
    "            |- sample2.fastq\n",
    "        |- processed/\n",
    "        |- results/\n",
    "    |- out/\n",
    "        |- logs/\n",
    "        |- reports/\n",
    "        |- figures/\n",
    "        |- temp/\n",
    "    |- rules/\n",
    "        |- preprocess.smk\n",
    "        |- analyze.smk\n",
    "        |- postprocess.smk\n",
    "        |- report.smk\n",
    "        |- log.smk\n",
    "        |- temp.smk\n",
    "    |- sandbox/\n",
    "    README.md\n",
    "    Snakefile\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create The Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define directory names\n",
    "directories = [\n",
    "    \"src/scripts/preprocessing\",\n",
    "    \"src/scripts/analysis\",\n",
    "    \"src/scripts/postprocessing\",\n",
    "    \"src/scripts/utilities\",\n",
    "    \"src/modules\",\n",
    "    \"src/config\",\n",
    "    \"src/lib\",\n",
    "    \"data/raw\",\n",
    "    \"data/processed\",\n",
    "    \"data/results\",\n",
    "    \"out/logs\",\n",
    "    \"out/reports\",\n",
    "    \"out/figures\",\n",
    "    \"out/temp\",\n",
    "    \"rules\",\n",
    "    \"sandbox\"\n",
    "]\n",
    "\n",
    "# Create directories\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Create sample data for sample1.fastq and sample2.fastq\n",
    "sample1_content = \"\"\"\\\n",
    "@Sample1_1\n",
    "ACTGACTGACTGACTGACTGACTGACTGACTGACTGACTGACTGACTG\n",
    "+\n",
    "!\"\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNO\n",
    "\n",
    "@Sample1_2\n",
    "CAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGT\n",
    "+\n",
    "!\"\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNO\n",
    "\"\"\"\n",
    "\n",
    "sample2_content = \"\"\"\\\n",
    "@Sample2_1\n",
    "GCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTA\n",
    "+\n",
    "!\"\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNO\n",
    "\n",
    "@Sample2_2\n",
    "TCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGA\n",
    "+\n",
    "!\"\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNO\n",
    "\"\"\"\n",
    "\n",
    "# Write sample data to files\n",
    "with open(\"data/raw/sample1.fastq\", \"w\") as file:\n",
    "    file.write(sample1_content)\n",
    "\n",
    "with open(\"data/raw/sample2.fastq\", \"w\") as file:\n",
    "    file.write(sample2_content)\n",
    "\n",
    "# Create empty files\n",
    "empty_files = [\n",
    "    \"src/scripts/preprocessing/preprocess.py\",\n",
    "    \"src/scripts/analysis/analyze.py\",\n",
    "    \"src/scripts/postprocessing/postprocess.py\",\n",
    "    \"src/scripts/utilities/log.py\",\n",
    "    \"src/scripts/utilities/report.py\",\n",
    "    \"rules/preprocess.smk\",\n",
    "    \"rules/analyze.smk\",\n",
    "    \"rules/postprocess.smk\",\n",
    "    \"rules/log.smk\",\n",
    "    \"rules/report.smk\",\n",
    "    \"rules/temp.smk\",\n",
    "    \"README.md\",\n",
    "    \"Snakefile\"\n",
    "]\n",
    "\n",
    "for file_path in empty_files:\n",
    "    if not os.path.exists(file_path):\n",
    "        open(file_path, \"w\").close()\n",
    "\n",
    "print(\"Directory structure created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, that the structure is ready, let's fill out the files with their corresponding codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***rules/preprocess.smk***\n",
    "```python\n",
    "rule preprocess:\n",
    "    input:\n",
    "        \"data/raw/{sample}.fastq\"\n",
    "    output:\n",
    "        \"data/processed/{sample}_preprocessed.fastq\"\n",
    "    shell:\n",
    "        \"python src/scripts/preprocessing/preprocess.py {input} {output}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***rules/analyze.smk***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rule analyze:\n",
    "    input:\n",
    "        \"data/processed/{sample}_preprocessed.fastq\"\n",
    "    output:\n",
    "        \"data/results/{sample}_analysis.txt\"\n",
    "    shell:\n",
    "        \"python src/scripts/analysis/analyze.py {input} {output}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***rules/postprocess.smk***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rule postprocess:\n",
    "    input:\n",
    "        \"data/results/{sample}_analysis.txt\"\n",
    "    output:\n",
    "        \"data/results/{sample}_gc_content.txt\"\n",
    "    shell:\n",
    "        \"python src/scripts/postprocessing/postprocess.py {input} {output}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***rules/report.smk***\n",
    "```python\n",
    "rule report:\n",
    "    input:\n",
    "        \"data/results/{sample}_gc_content.txt\"\n",
    "    output:\n",
    "        \"out/reports/{sample}_report.txt\"\n",
    "    shell:\n",
    "        \"python src/scripts/utilities/report.py {input} {output}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***rules/log.smk***\n",
    "```python\n",
    "rule log:\n",
    "    input:\n",
    "        \"data/raw/{sample}.fastq\"\n",
    "    output:\n",
    "        \"out/logs/{sample}.log\"\n",
    "    shell:\n",
    "        \"python src/scripts/utilities/log.py 'Processing {wildcards.sample}' {output}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***rules/temp.smk***\n",
    "```python\n",
    "rule temp_file:\n",
    "    input:\n",
    "        \"data/raw/{sample}.fastq\"\n",
    "    output:\n",
    "        temp(\"out/temp/{sample}_temp.txt\")\n",
    "    shell:\n",
    "        \"echo 'Temporary file for {wildcards.sample}' > {output}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These are the rules that we want to follow in this workflow. You can replace the contets of the mentioned files with these codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***src/scripts/preprocessing/preprocess.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script reads a FASTQ file, processes its content by converting all characters to uppercase, \n",
    "and writes the processed content to a new file.\n",
    "\n",
    "Usage:\n",
    "    python preprocess.py input.fastq output.fastq\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "def preprocess(input_file, output_file):\n",
    "    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "        for line in f_in:\n",
    "            # Example preprocessing: convert to uppercase\n",
    "            f_out.write(line.upper())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    preprocess(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***src/scripts/analysis/analyze.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script reads a preprocessed FASTQ file and counts the occurrences of each nucleotide ('A', 'T', 'C', 'G').\n",
    "It writes the counts to an output file.\n",
    "\n",
    "Usage:\n",
    "    python analyze.py input_preprocessed.fastq output_analysis.txt\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "def analyze(input_file, output_file):\n",
    "    counts = {'A': 0, 'T': 0, 'C': 0, 'G': 0}\n",
    "    with open(input_file, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            if not line.startswith('@') and not line.startswith('+'):\n",
    "                for char in line.strip():\n",
    "                    if char in counts:\n",
    "                        counts[char] += 1\n",
    "    with open(output_file, 'w') as f_out:\n",
    "        for nucleotide, count in counts.items():\n",
    "            f_out.write(f\"{nucleotide}: {count}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    analyze(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***src/scripts/postprocessing/postprocess.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script reads an analysis file containing nucleotide counts, calculates the GC content,\n",
    "and writes this information to an output file.\n",
    "\n",
    "Usage:\n",
    "    python postprocess.py input_analysis.txt output_gc_content.txt\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "def calculate_gc_content(input_file, output_file):\n",
    "    total_bases = 0\n",
    "    gc_bases = 0\n",
    "    with open(input_file, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            if line.startswith('A:') or line.startswith('T:') or line.startswith('C:') or line.startswith('G:'):\n",
    "                nucleotide, count = line.strip().split(': ')\n",
    "                count = int(count)\n",
    "                total_bases += count\n",
    "                if nucleotide in ['G', 'C']:\n",
    "                    gc_bases += count\n",
    "    gc_content = (gc_bases / total_bases) * 100 if total_bases > 0 else 0\n",
    "    with open(output_file, 'w') as f_out:\n",
    "        f_out.write(f\"GC Content: {gc_content:.2f}%\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    calculate_gc_content(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***src/scripts/utilities/log.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script logs a message with a timestamp to an output log file.\n",
    "\n",
    "Usage:\n",
    "    python log.py \"message\" output.log\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "def log_message(message, output_file):\n",
    "    with open(output_file, 'a') as f_out:\n",
    "        f_out.write(f\"{datetime.datetime.now()}: {message}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    message = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    log_message(message, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***src/scripts/utilities/report.py***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script generates a report based on the input GC content file and writes it to an output file.\n",
    "\n",
    "Usage:\n",
    "    python report.py input_gc_content.txt output_report.txt\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "def generate_report(input_file, output_file):\n",
    "    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "        f_out.write(\"Report\\n\")\n",
    "        f_out.write(\"======\\n\\n\")\n",
    "        for line in f_in:\n",
    "            f_out.write(line)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    generate_report(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the Snakefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Snakefile***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "include: \"rules/preprocess.smk\"\n",
    "include: \"rules/analyze.smk\"\n",
    "include: \"rules/postprocess.smk\"\n",
    "include: \"rules/report.smk\"\n",
    "include: \"rules/log.smk\"\n",
    "include: \"rules/temp.smk\"\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"data/results/{sample}_gc_content.txt\", sample=[\"sample1\", \"sample2\"]),\n",
    "        expand(\"out/logs/{sample}.log\", sample=[\"sample1\", \"sample2\"]),\n",
    "        expand(\"out/reports/{sample}_report.txt\", sample=[\"sample1\", \"sample2\"]),\n",
    "        expand(\"out/temp/{sample}_temp.txt\", sample=[\"sample1\", \"sample2\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 12\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob stats:\n",
      "job            count\n",
      "-----------  -------\n",
      "all                1\n",
      "analyze            2\n",
      "log                2\n",
      "postprocess        2\n",
      "preprocess         2\n",
      "report             2\n",
      "temp_file          2\n",
      "total             13\n",
      "\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule temp_file:\n",
      "    input: data/raw/sample1.fastq\n",
      "    output: out/temp/sample1_temp.txt\n",
      "    jobid: 11\n",
      "    reason: Missing output files: out/temp/sample1_temp.txt\n",
      "    wildcards: sample=sample1\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule preprocess:\n",
      "    input: data/raw/sample2.fastq\n",
      "    output: data/processed/sample2_preprocessed.fastq\n",
      "    jobid: 6\n",
      "    reason: Missing output files: data/processed/sample2_preprocessed.fastq\n",
      "    wildcards: sample=sample2\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule log:\n",
      "    input: data/raw/sample2.fastq\n",
      "    output: out/logs/sample2.log\n",
      "    jobid: 8\n",
      "    reason: Missing output files: out/logs/sample2.log\n",
      "    wildcards: sample=sample2\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule temp_file:\n",
      "    input: data/raw/sample2.fastq\n",
      "    output: out/temp/sample2_temp.txt\n",
      "    jobid: 12\n",
      "    reason: Missing output files: out/temp/sample2_temp.txt\n",
      "    wildcards: sample=sample2\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule preprocess:\n",
      "    input: data/raw/sample1.fastq\n",
      "    output: data/processed/sample1_preprocessed.fastq\n",
      "    jobid: 3\n",
      "    reason: Missing output files: data/processed/sample1_preprocessed.fastq\n",
      "    wildcards: sample=sample1\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule log:\n",
      "    input: data/raw/sample1.fastq\n",
      "    output: out/logs/sample1.log\n",
      "    jobid: 7\n",
      "    reason: Missing output files: out/logs/sample1.log\n",
      "    wildcards: sample=sample1\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 11.\u001b[0m\n",
      "\u001b[32m1 of 13 steps (8%) done\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 12.\u001b[0m\n",
      "\u001b[32m2 of 13 steps (15%) done\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 6.\u001b[0m\n",
      "\u001b[32m3 of 13 steps (23%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule analyze:\n",
      "    input: data/processed/sample2_preprocessed.fastq\n",
      "    output: data/results/sample2_analysis.txt\n",
      "    jobid: 5\n",
      "    reason: Missing output files: data/results/sample2_analysis.txt; Input files updated by another job: data/processed/sample2_preprocessed.fastq\n",
      "    wildcards: sample=sample2\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 3.\u001b[0m\n",
      "\u001b[32m4 of 13 steps (31%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule analyze:\n",
      "    input: data/processed/sample1_preprocessed.fastq\n",
      "    output: data/results/sample1_analysis.txt\n",
      "    jobid: 2\n",
      "    reason: Missing output files: data/results/sample1_analysis.txt; Input files updated by another job: data/processed/sample1_preprocessed.fastq\n",
      "    wildcards: sample=sample1\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 8.\u001b[0m\n",
      "\u001b[32m5 of 13 steps (38%) done\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 5.\u001b[0m\n",
      "\u001b[32m6 of 13 steps (46%) done\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 7.\u001b[0m\n",
      "\u001b[32m7 of 13 steps (54%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule postprocess:\n",
      "    input: data/results/sample2_analysis.txt\n",
      "    output: data/results/sample2_gc_content.txt\n",
      "    jobid: 4\n",
      "    reason: Missing output files: data/results/sample2_gc_content.txt; Input files updated by another job: data/results/sample2_analysis.txt\n",
      "    wildcards: sample=sample2\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 2.\u001b[0m\n",
      "\u001b[32m8 of 13 steps (62%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule postprocess:\n",
      "    input: data/results/sample1_analysis.txt\n",
      "    output: data/results/sample1_gc_content.txt\n",
      "    jobid: 1\n",
      "    reason: Missing output files: data/results/sample1_gc_content.txt; Input files updated by another job: data/results/sample1_analysis.txt\n",
      "    wildcards: sample=sample1\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 4.\u001b[0m\n",
      "\u001b[32m9 of 13 steps (69%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule report:\n",
      "    input: data/results/sample2_gc_content.txt\n",
      "    output: out/reports/sample2_report.txt\n",
      "    jobid: 10\n",
      "    reason: Missing output files: out/reports/sample2_report.txt; Input files updated by another job: data/results/sample2_gc_content.txt\n",
      "    wildcards: sample=sample2\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 1.\u001b[0m\n",
      "\u001b[32m10 of 13 steps (77%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mrule report:\n",
      "    input: data/results/sample1_gc_content.txt\n",
      "    output: out/reports/sample1_report.txt\n",
      "    jobid: 9\n",
      "    reason: Missing output files: out/reports/sample1_report.txt; Input files updated by another job: data/results/sample1_gc_content.txt\n",
      "    wildcards: sample=sample1\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 10.\u001b[0m\n",
      "\u001b[32m11 of 13 steps (85%) done\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 9.\u001b[0m\n",
      "\u001b[32m12 of 13 steps (92%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mlocalrule all:\n",
      "    input: data/results/sample1_gc_content.txt, data/results/sample2_gc_content.txt, out/logs/sample1.log, out/logs/sample2.log, out/reports/sample1_report.txt, out/reports/sample2_report.txt, out/temp/sample1_temp.txt, out/temp/sample2_temp.txt\n",
      "    jobid: 0\n",
      "    reason: Input files updated by another job: data/results/sample1_gc_content.txt, out/temp/sample1_temp.txt, out/temp/sample2_temp.txt, out/reports/sample1_report.txt, out/logs/sample2.log, data/results/sample2_gc_content.txt, out/logs/sample1.log, out/reports/sample2_report.txt\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Sun May 26 08:09:16 2024]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m13 of 13 steps (100%) done\u001b[0m\n",
      "\u001b[33mRemoving temporary output out/temp/sample1_temp.txt.\u001b[0m\n",
      "\u001b[33mRemoving temporary output out/temp/sample2_temp.txt.\u001b[0m\n",
      "\u001b[33mComplete log: .snakemake/log/2024-05-26T080915.977780.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! snakemake --cores all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you are getting an error like this `AttributeError: module 'pulp' has no attribute 'list_solvers'. Did you mean: 'listSolvers'?`, try downgrading the `pulp` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install PuLP==2.7.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
