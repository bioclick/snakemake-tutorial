# What is Snakemake
Snakemake is a workflow management system used primarily in bioinformatics and data science for defining and executing complex data analysis workflows. It is designed to facilitate reproducibility, scalability, and flexibility in scientific workflows. Here is a detailed explanation of Snakemake, covering its key features, components, and usage alongside with sptep by step guide for its setup.

## Organization of files and folders
Using a specified folder structure helps to understand the project better. Therefore, it is recommended to follow some conventions in order to keep things nice and neat.

Your project structure should look like this:
```bash
./
    |- src/
    |- data/
    |- out/
    |- rules/
    |- sandbox/
    README.md
    Snakefile
```

- `Root (./):` This is the main folder of the project. Everything that is used or produced for the project should be located in a this folder.

- `src:` This is the ‘source’ folder. It contains all of your code files you develop during your analysis and the original datasets you begin your analysis with.

- `data:` Contains all data files, organized by their state in the workflow (raw, processed, results).

- `out:` This is the output directory. We will put anything that we create by running a Python script.

- `rules:` Rules are defined in the Snakefile, which serves as the central script that orchestrates the execution of the workflow. However, for larger and more complex workflows, it's often beneficial to organize rules into separate files and directories to improve readability and maintainability.

- `sandbox:` As we work through our project, we will want to explore new ideas and test out how to code them. While we play with these bits and pieces of code, we save them in the sandbox. Separating them from src means we know that scripts in here are ‘under development’. When they are finalized, we can move them into src.

- `README.md:` A README is a file that introduces and explains the project. We should write information that explains what the project is about and how someone can run the scripts developed for the project in this file. We also recommend providing installation instructions to clarify exactly what needs to be installed to run the project.

- `Snakefile:` We will use the Snakefile to write the steps needed that run all the scripts in the project. 

You can check your project's contents by using `ls` command:
```bash
ls
```

### `src` Directory In-Depth
The `src` directory typically contains the source code and scripts required to execute various steps in your workflow. The subdirectories can be organized based on the different types of scripts and their purposes.
```bash
./
    |src/
        |- scripts/
            |- preprocessing/
            |- analysis/
            |- postprocessing/
            |- utilities/
        |- modules/
        |- config/
        |- lib/
```

- **`scripts:`** This subdirectory can contain various scripts used in the workflow. These might include:

  - **preprocessing:** Scripts for data preprocessing steps.
  - **analysis:** Scripts for data analysis.
  - **postprocessing:** Scripts for post-processing results.
  - **utilities:** Utility scripts that provide common functions used across multiple steps.

- **`modules:`** If using modular components, this directory can hold reusable workflow modules, each with its own Snakefile or set of rules.

- **`config:`** Configuration files (YAML, JSON, or other formats) that store parameters, paths, and other settings needed for the workflow.

- **`lib:`** Custom libraries or modules written in Python or other languages that provide additional functionality used in the workflow scripts.

### `data` Directory In-Depth
The `data` directory is kept separate to clearly distinguish between code and data. This separation helps in maintaining a clean and organized project layout, where source code and data are managed independently.
```bash
./
    |data/
        |- raw/
        |- processed/
        |- results/
```
- **`raw:`**  Original, unprocessed data.
- **`processed:`** Data that has been cleaned or transformed.
- **`results:`** Final output data from analyses.

### `out` Directory In-Depth
The `out` directory typically contains the results and output files generated by the workflow. The subdirectories can be organized based on the stages of the workflow, types of output.
```bash
./
    |out/
        |- logs/
        |- reports/
        |- figures/
        |- temp/
```
- **`logs:`**  Log files for tracking workflow execution.
- **`reports:`** Generated reports.
- **`figures:`** Plots and visualizations.
- **`temp:`**  Temporary files.

### `rules` Directory In-Depth
For larger projects, organizing rules into separate files is a good practice. Notice that rule files are in `.smk` format.
```bash
./
    |rules/
        |- example_rule.smk
```
You can add as many rules as you want here. Then, we can include these rules using the `include` or `module` directive in the `Snakemake` file.

## Rules In-Depth
Rules are the fundamental building blocks of a Snakemake workflow. Each rule typically includes:
- **`input`:** Specifies the input files needed for the rule.
- **`output`:** Specifies the output files generated by the rule.
- **`shell`:** Contains the command(s) to be executed.
- **`params`**: Additional parameters for the rule.
- **`resources`:** Resource requirements for the rule.
- **`run`:** An alternative to shell, allows embedding Python code directly in the rule.

Below, you can see an example of a rule file (`.smk`):

***example_rule.smk***
```python
# rules/example_rule.smk

rule example_rule:
    # Specifies the input files needed for the rule
    input:
        "data/raw/input_data.csv"

    # Specifies the output files generated by the rule
    output:
        "data/processed/processed_data.csv"

    # Additional parameters for the rule
    params:
        threshold = 0.1,
        log_file = "logs/example_rule.log"

    # Resource requirements for the rule
    resources:
        mem_mb = 1024,
        time = "01:00:00"

    # Contains the command(s) to be executed
    shell:
        """
        python scripts/process_data.py {input} {output} {params.threshold} > {params.log_file}
        """

    # An alternative to shell, allows embedding Python code directly in the rule
    run:
        import pandas as pd
        
        # Load input data
        input_data = pd.read_csv(input[0])
        
        # Perform some processing
        processed_data = input_data[input_data['value'] > params.threshold]
        
        # Save processed data to output file
        processed_data.to_csv(output[0], index=False)
        
        # Log message
        with open(params.log_file, 'a') as log:
            log.write("Processing completed successfully.\n")

```

## Snakefile:
The core of a Snakemake workflow is the Snakefile, which contains rules defining the workflow steps. Each rule specifies the input and output files, the commands to be executed, and the resources required. After defining your rules, in your main Snakefile, you can include the rule file using the include directive:

***Snakefile***
```python
# Include the rule file(s)
include: "rules/example_rule.smk"

# Rule to aggregate final outputs
rule all:
    input:
        "data/processed/processed_data.csv"
```

Moreover, if you have multiple rules, namely `example_rule_2.smk` and `example_rule_3.smk`, you can simply add them to your Snakefile as well:
***Snakefile***
```python
# Include the rule file(s)
include: "rules/example_rule.smk"
include: "rules/example_rule_2.smk"
include: "rules/example_rule_3.smk"


# Rule to aggregate final outputs
rule all:
    input:
        "data/processed/processed_data.csv"
```

## Running a Snakemake Workflow
To run the workflow defined in the Snakefile, use the following command:
```bash
snakemake
```
This command will execute the workflow, creating the output files specified in the all rule. Snakemake automatically determines the order of rule execution based on input-output dependencies.

## Flags and Options
Snakemake offers a variety of command-line flags and options to customize the execution of workflows. Here are the key options available when running Snakemake:

```bash
snakemake [OPTIONS] [TARGETS]
```

### 1. Workflow Execution
- `--snakefile [FILE]`: Specify the path to the Snakefile. Default is Snakefile.
- `--cores [N]`: Use at most N CPU cores/jobs in parallel. Default is 1. Use --cores all to use all available cores.
- `--jobs [N]`: Deprecated in favor of --cores.
- `--directory [DIR]`: Specify the working directory. Default is the current directory.
- `--dry-run or -n`: Perform a dry run, only showing what would be done without actually executing the workflow.
- `--rerun-incomplete`: Re-run all jobs that are incomplete or have missing output files.
- `--forceall or -F`: Force the execution of the entire workflow.
- `--touch`: Touch output files (mark them as updated) without actually running the commands.

### 2. Logging and Reporting
- `--printshellcmds or -p`: Print out the shell commands that will be executed.
- `--reason or -r`: Print the reason for each executed rule.
- `--summary`: Print a summary of the workflow.
- `--report [FILE]`: Generate an HTML report of the workflow execution.
- `--dag`: Print the directed acyclic graph (DAG) of jobs in the workflow.
- `--rulegraph`: Print the rule graph, showing dependencies between rules.
- `--d3dag`: Print the DAG in D3.js format for visualization in a web browser.

### 3. Resource Management
- `--resources [KEY=VALUE]`: Specify resources for rules. For example, --resources mem_mb=1024.
- `--max-jobs-per-second [N]`: Limit the number of jobs started per second to N.
- `--max-status-checks-per-second [N]`: Limit the number of status checks per second to N.

### 4. Input/Output Handling
- `--input [FILES]`: Specify input files to start the workflow from.
- `--output [FILES]`: Specify output files to be produced by the workflow.
- `--delete-all-output or -d`: Delete all output files of the workflow.
- `--timestamp`: Add a timestamp to the name of output files.

### 5. Cluster Execution
- `--cluster [CMD]`: Execute rules on a cluster using the specified submission command.
- `--cluster-config [FILE]`: Specify a JSON or YAML file with cluster configuration.
- `--jobs [N]`: Specify the maximum number of simultaneously submitted cluster jobs.
- `--drmaa [ARGS]`: Use DRMAA to submit jobs to a cluster.
- `--jobname [NAME]`: Specify the job name pattern for cluster jobs.

### 6. Other Options
- `--config [KEY=VALUE]`: Set or override configuration values.
- `--configfile [FILE]`: Specify a configuration file in JSON or YAML format.
- `--profile [DIR]`: Specify a profile directory for configuration settings.
- `--use-conda`: Enable the use of Conda environments for rule dependencies.
- `--conda-prefix [DIR]`: Specify a directory to store Conda environments.
- `--use-singularity`: Enable the use of Singularity containers for rule dependencies.
- `--singularity-prefix [DIR]`: Specify a directory to store Singularity images.
- `--lint`: Lint the workflow for common errors.
- `--version`: Show the Snakemake version.
